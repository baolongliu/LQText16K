
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SHIP</title>
    <style>
        * {
            /* font-family:  Century Gothic,CenturyGothic,AppleGothic,sans-serif;; */
            font-family: 'Times New Roman', Times, serif;
            /* font-style: oblique; */
            margin: 0%;
            padding: 0%;
        }
        html {
            scroll-behavior: smooth;
        }

        .container {
            text-align: center;
            width: 1000px;
            margin-left:auto;  
            margin-right:auto;
        }
        .container > div {
            width: 1000px;
            display: inline-block;
            margin-top: 20px;
        }
        .title {
            font-size: 40px;
            font-weight: 400;
            margin-bottom: 10px;
        }
        p.authors {
            font-size: 20px;
            margin-bottom: 10px;
        }
        p.affi {
            padding: 0px;
            margin: 0px;
            font-size: 20px;
        }
        .model_img {
            margin-top: 20px;
            width: 900px;
        }
        span.title {
            display: block;
            text-align: left;
            padding-bottom: 5px;
            font-size: 30px;
            border-bottom: 1.5px solid rgb(229, 229, 229);
        }
        span.title_content {
            padding-top: 5px;
            display: block;
            text-align: justify;
            padding-bottom: 5px;
            font-size: 17px;
        }
        .performance_container {
            display: flex;
            justify-content: space-around;
        }
        span.dataset {
            display: block;
            font-size: 25px;
            padding-top: 20px;
            padding-bottom: 10px;
            font-weight: 400;
        }
        img.performance_img {
            width: 450px;
        }
        img.group_img {
            width: 1000px;
        }
        .sources_container {
            display: flex;
            justify-content: space-around;
            margin-bottom: 20px;
        }
        img.source_img {
            padding-top: 30px;
            display: block;
            width: 100px;
            transition: 0.5s;
        }
        img.source_img:hover {
            transform: scale(120%, 120%);
            -webkit-transform: scale(120%, 120%);
            -moz-transform: scale(120%, 120%);
        }
        span.source_title {
            display: block;
            margin-top: 15px;
            font-size: 25px;
        }
        .bib {
            text-align: left;
            font-weight: 400;
        }
        pre {
            text-align: left;
            display: block;
            padding: 9.5px;
            margin: 0 0 10px;
            font-size: 13px;
            line-height: 1.428571429;
            color: #333;
            word-break: break-all;
            word-wrap: break-word;
            background-color: #f5f5f5;
            border: 1px solid #ccc;
            border-radius: 4px;
        }
        table.dataset {
            margin: auto;
            border-collapse: collapse;
            border-top: 2px solid black;
            border-bottom: 2px solid black; 
            margin-top: 2px;
        }
        tr.split {
            border-collapse: collapse;
            border-top: 1px solid black;
        }
        tr.model_name {
            font-size: 15px;
        }
        td {
            padding-top: 3px;
            padding-bottom: 3px;
            padding-left: 8px;
            padding-right: 8px;
        }
        th {
            padding-top: 3px;
            padding-bottom: 3px;
            padding-left: 8px;
            padding-right: 8px;
        }
        a {
            text-decoration: none;
        }
        a:visited, a:hover, a:link, a:active {
            color: #333;
        }
        ol.reference_content {
            margin-top: 10px;
            padding-left: 1.2em;
            text-align: justify;
            font-size: 15px;
            /* font-family:  Century Gothic,CenturyGothic,AppleGothic,sans-serif;; */
        }
        li {
            padding-bottom: 15px;
        }
        ul {
            margin-top: 10px;
            padding-left: 1.2em;
            text-align: justify;
            font-size: 17px;
        }
    </style>

</head>
<body>
    <div class="container">
        <div class="header">
            <h3 class="title">Going Deeper into Weak Feature Text Recognition
                in the Wild: A New Benchmark Dataset</h3>
            <p class="authors">Baolong Liu<sup>1</sup> &nbsp;&nbsp;&nbsp;&nbsp;
                Ruiqing Yang<sup>2</sup> &nbsp;&nbsp;&nbsp;&nbsp;
                Wenhao Xu<sup>3</sup>  &nbsp;&nbsp;&nbsp;&nbsp;
                Roukai Huang<sup>4</sup> &nbsp;&nbsp;&nbsp;&nbsp;
                Jiangfeng Dong<sup>5</sup> &nbsp;&nbsp;&nbsp;&nbsp;
                Xun Wang<sup>6</sup>
            </p>
            <p class="affi">¹School of Computer and Information Engineering, Zhejiang Gongshang University</p>
            <!-- <p class="affi">²School of Information Science and Technology, University of Science and Technology of China</p> -->
            <!-- <p class="affi">³Key Lab of Data Engineering and Knowledge Engineering, Renmin University of China</p> -->
        </div>
        <div class="sources">
            <span class="title"></span>
            <div class="sources_container">
                <div class="source">
                    <a href="#" class="source_img" target="_blank">
                        <img src="imgs/paper.svg" alt="" class="source_img">
                    </a>
                    <span class="source_title">Paper</span>
                </div>
                <div class="source">
                    <a href="PRVR_slides.pdf" class="source_img" target="_blank">
                        <img src="imgs/ppt.svg" alt="" class="source_img">
                    </a>
                    <span class="source_title">Slides</span>
                </div>
                <div class="source">
                    <a href="https://github.com/HuiGuanLab/ms-sl/tree/main/dataset" class="source_img" target="_blank">
                        <img src="imgs/data.svg" alt="" class="source_img">
                    </a>
                    <span class="source_title">Data</span>
                </div>
                <div class="source">
                    <a href="https://github.com/HuiGuanLab/ms-sl" class="source_img" target="_blank">
                        <img src="imgs/code.svg" alt="" class="source_img">
                    </a>
                    <span class="source_title">Code</span>
                </div>
            </div>
            <div id="abs"></div>
            <pre ><tt>Todo</tt></pre>
            
        </div>
        <div class="abstract">
            <span class="title">Abstract</span>
            <span class="title_content">
                TODO
            </span>
        </div>
        <div class="formulation">
            <span class="title">Introduction</span>
            <span class="title_content">
                Reading texts from natural images, also referred as
                Scene Text Recognition (STR), is of great importance
                in many application areas. Automatic Ship License Plate
                Recognition (ASLPR) is firstly to localize Ship License Plate
                (SLP) regions in the visible light images, then recognize the
                text in those regions [1]–[4], and is a newly-introduced sub-
                task of STR. As unique identities, SLPs play a significant role
                in ship-to-ship communication, shore-to-ship identification and
                waterway traffic monitoring in occasions such as canlas, ports
                and harbors et al. . A related task is car license plate recog-
                nition [5], which has been studied for years, and is already
                widely used in many real scenes, e.g. autonomous vehicles,
                vehicles tracking and smart city. Just as car license plate
                recognition is to landway transportation, so SLP recognition
                is to waterway transportation. ASLPR is an important issue in
                waterway intelligent transportation systems.
            </span>
            <span class="title_content">
                Recently, a few works concerning the detecting and rec-
                ognizing of SLPs are proposed [1]–[4]. However, the re-
                searches are very preliminary, and no public accessible SLPs
                dataset is released also, hindering the advance of the field.
                Although a number of benchmark datasets regarding STR have
                been proposed [6]–[11] in the past years, it is frequent that,
                when adopted to real applications, the cutting-edge methods
                optimized on the current datasets often show much poorer
                performance or generalization-ability than that reported in the
                literature. For example, the newly presented state-of-the-art
                (SOTA) method in [12] obtains a mean word-level recognition
                accuracy of 96.0% already on six main-stream regular or
                irregular text recognition datasets i.e. IIIT5k [6], CUTE80 [7],
                SVT [8], SVTP [9], ICDAR 2013 [10] and ICDAR 2015 [11],
                but only achieves an accuracy of 35% in our proposed SLPs
                dataset. Obviously, there are huge gaps between research and
                real scenes, trapping those endeavoring in STR field for a long
                time
            </span>
            <!-- <img src="imgs/examples_new.png" alt="" class="group_img">
            <span class="title_content">
                An untrimmed video is considered to be partially relevant w.r.t. a given textual
                query as long as the video contains a (short) moment relevant w.r.t.
                the query, see the figure above. Two textual queries partially relevant to a given video. Only a specific moment in the video is relevant to the
                corresponding query, while the other frames are irrelevant.
            </span> -->
        </div>
        <div class="method">
            <span class="title">Data Visualization</span>
            <!-- <span class="title_content">
                We formulate the PRVR subtask as a MIL problem, simultaneously viewing a video as a bag of video clips and a bag of video
                frames. Clips and frames represent video content at different temporal scales. Based on multi-scale video representation, we propose
                MS-SL to compute the relevance between videos and queries in a
                coarse-to-fine manner. The structure of our proposed model is shown in the figure below.
            </span> -->
            <img src="imgs/data1.svg" alt="" class="">
        </div>
        <div class="performance">
            <span class="title">Performance Comparision</span>
            <span class="title_content">
                <!-- As videos in popular T2VR datasets such as MSR-VTT, MSVD and VATEX are supposed to be fully relevant
                to the queries, they are not suited for our experiments. Here, we
                re-purpose three datasets commonly used for VCMR, <em>i.e.</em>, TVR,
                Activitynet Captions, and Charades-STA, considering their
                natural language queries partially relevant with the corresponding
                videos (a query is typically associated with a specific moment in
                a video). The results on the three datasets are summarized in the tables below. -->
            </span>
            <div class="datasets">
                <span class="title_content">
                    We adopted the state of the art text detection model and text recognition model to carry out relevant experiments on our dataset.
                </span>
                <span class="dataset">Text Detection Algorithm:</span>
                
                <table class="dataset" style="width: 500px;">
                    <tr>
                        <th colspan="5" align="left">Model</th>
                        <th>Year</th>
                        <th>Publisher</th>
                        <th>Precision</th>
                        <th>Recall</th>
                        <th>F-Measure</th>
                    </tr>
                    <!-- <tr class="split">
                        <td align="left"><em>T2VR models:</em></td>
                    </tr> -->
                    <tr class="model_name split">
                        <td colspan="5" align="left" >DB++ <a href="#db++">[1]</a></td>
                        <td>2022</td>
                        <td>TPAMI</td>
                        <td>91.63</td>
                        <td>35.30</td>
                        <td>50.97</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left">I3CL <a href="#i3cl">[2]</a></td>
                        <td>2021</td>
                        <td>IJCV</td>
                        <td>49.70</td>
                        <td>60.60</td>
                        <td>54.66</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left">EAST <a href="#east">[3]</a></td>
                        <td>2017</td>
                        <td>CVPR</td>
                        <td>80.21</td>
                        <td>49.37</td>
                        <td>61.30</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left" >FCENet <a href="#fcenet">[4]</a></td>
                        <td>2021</td>
                        <td>CVPR</td>
                        <td>80.92</td>
                        <td>50.82</td>
                        <td>62.43</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left">DPText-DETR <a href="#dptext-detr">[5]</a></td>
                        <td>2023</td>
                        <td>AAAI</td>
                        <td>69.53</td>
                        <td>72.17</td>
                        <td>70.82</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left" >TESTR <a href="#testr">[6]</a></td>
                        <td>2022</td>
                        <td>CVPR</td>
                        <td>80.09</td>
                        <td>76.83</td>
                        <td>78.43</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left">TextBPN++ <a href="#textbpn++">[7]</a></td>
                        <td>2022</td>
                        <td>TPAMI</td>
                        <td>88.25</td>
                        <td>76.70</td>
                        <td>82.07</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left">PAN <a href="#pan">[8]</a></td>
                        <td>2019</td>
                        <td>ICCV</td>
                        <td>91.76</td>
                        <td>88.20</td>
                        <td>89.94</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left">YOLOv5+CSL </td>
                        <td> </td>
                        <td> </td>
                        <td>88.63</td>
                        <td>86.58</td>
                        <td>90.77</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left">YOLOv8 </td>
                        <td> </td>
                        <td> </td>
                        <td>90.93</td>
                        <td>93.83</td>
                        <td>92.14</td>
                    </tr>
                </table>
            </div>
            <div class="datasets">
                <span class="dataset">Text Recognition Algorithm:</span>
                <table class="dataset" style="width: 600px;">
                    <tr>
                        <th colspan="6" align="left">Method</th>
                        <th>Year</th>
                        <th>Publisher</th>
                        <th>LM</th>
                        <th>Params(M)</th>
                        <th>FPS</th>
                        <th>NED</th>
                        <th>Accuary</th>
                    </tr>
                    <tr class="split">
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left" >CRNN <a href="#CRNN">[9]</a></td>
                        <td>2016</td>
                        <td>TPAMI</td>
                        <td>-</td>
                        <td>8.59</td>
                        <td>144.93</td>
                        <td>43.64</td>
                        <td>5.29</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">TRBA <a href="#TRBA">[10]</a></td>
                        <td>2019</td>
                        <td>ICCV</td>
                        <td>-</td>
                        <td>33.65</td>
                        <td>44.43</td>
                        <td>64.76</td>
                        <td>30.26</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">ViTSTR <a href="#ViTSTR">[11]</a></td>
                        <td>2021</td>
                        <td>ICDAR</td>
                        <td>-</td>
                        <td>85.83</td>
                        <td>83.33</td>
                        <td>76.67</td>
                        <td>44.60</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left" >SVTR <a href="#SVTR">[12]</a></td>
                        <td>2022</td>
                        <td>IJCAI</td>
                        <td>-</td>
                        <td>22.33</td>
                        <td>69.50</td>
                        <td>80.23</td>
                        <td>45.16</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">SemiMTR <a href="#SemiMTR">[13]</a></td>
                        <td>2022</td>
                        <td>-</td>
                        <td>&#10003;</td>
                        <td>37.81</td>
                        <td>5.92</td>
                        <td>75.13</td>
                        <td>49.76</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left" >PARSeq <a href="#PARSeq">[14]</a></td>
                        <td>2022</td>
                        <td>ECCV</td>
                        <td>-</td>
                        <td>24.20</td>
                        <td>34.13</td>
                        <td>77.86</td>
                        <td>56.52</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">MATRN<a href="#MATRN">[15]</a></td>
                        <td>2022</td>
                        <td>ECCV</td>
                        <td>&#10003;</td>
                        <td>45.77</td>
                        <td>16.93</td>
                        <td>79.74</td>
                        <td>59.39</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">ABINet<a href="#ABINet">[16]</a></td>
                        <td>2021</td>
                        <td>CVPR</td>
                        <td>&#10003;</td>
                        <td>37.81</td>
                        <td>24.14</td>
                        <td>80.28</td>
                        <td>59.71</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">MASTER <a href="#MASTER">[17]</a></td>
                        <td>2021</td>
                        <td>PR</td>
                        <td>-</td>
                        <td>69.86</td>
                        <td>7.86</td>
                        <td>80.53</td>
                        <td>63.17</td>
                    </tr>
                    <tr class="model_name split">
                        <td colspan="6" align="left">Ours<a href="#Ours">[18]</a></td>
                        <td>2023</td>
                        <td>-</td>
                        <td>-</td>
                        <td>122</td>
                        <td>34.41</td>
                        <td>85.92</td>
                        <td>66.01</td>
                    </tr>
                </table>
            </div>
        </div>
        <div class="group">
            <span class="title">Our SOTA Algorithm Visualization</span>
            <span class="title_content">
                To gain a further understanding of the individual models, we
                define <em>moment-to-video ratio</em> (M/V) for query, which is measured
                by its corresponding moment's length ratio in the entire video. According
                to M/V, queries can be automatically classified into different groups,
                which enables a fine-grained analysis of how a specific model responds to the different types of queries. 
            </span>
            <img id="mypoly" src="imgs/poly_raw.jpg" alt="" class="group_img">
            <span class="title_content">
                The performance in
                the group with the lowest M/V is the smallest, while the group with
                the highest M/V is the largest. The result allows us to conclude that
                the current video retrieval baseline models better address queries
                of larger relevance to the corresponding video. By contrast, the
                performance we achieved is more balanced in all groups. 
                This result shows that our proposed model is less sensitive to irrelevant
                content in videos.
            </span>
        </div>
        <div class="acknowledgement">
            <span class="title">Acknowledgement</span>
            <span class="title_content" >
                This work was supported by the National
                Key R&D Program of China (2018YFB1404102), NSFC (62172420,
                61902347, 61976188, 62002323), the Public Welfare Technology Research Project of Zhejiang Province (LGF21F020010), the Open
                Projects Program of the National Laboratory of Pattern Recognition,
                the Fundamental Research Funds for the Provincial Universities of
                Zhejiang, and Public Computing Cloud of RUC.
            </span>
        </div>
        <div class="contact">
            <span class="title">Contact</span>
            <ul>
                <li>Baolong Liu: <b><em>liubaolongx@gmail.com</em></li></b> 
                <li>Ruiqing Yang: <b><em>yrq991121@gmail.com</em></li></b> 
            </ul>
        </div>
        <div class="reference">
            <span class="title">Reference</span>
            <ol class="reference_content">
                <li id="db++">
                    Liao M, Zou Z, Wan Z, et al. Real-time scene text detection with differentiable binarization and adaptive scale fusion[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022, 45(1): 919-931.
                </li>
                <li id="i3cl">
                    Du B, Ye J, Zhang J, et al. I3cl: intra-and inter-instance collaborative learning for arbitrary-shaped scene text detection[J]. International Journal of Computer Vision, 2022, 130(8): 1961-1977. 
                </li>
                <li id="east">
                    Zhou X, Yao C, Wen H, et al. East: an efficient and accurate scene text detector[C]//Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 2017: 5551-5560. 
                </li>
                <li id="fcenet">
                    Zhu Y, Chen J, Liang L, et al. Fourier contour embedding for arbitrary-shaped text detection[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 3123-3131. 
                </li>
                <li id="dptext-detr">
                    Ye M, Zhang J, Zhao S, et al. Dptext-detr: Towards better scene text detection with dynamic points in transformer[J]. arXiv preprint arXiv:2207.04491, 2022. 
                </li>
                <li id="testr">
                    Zhang X, Su Y, Tripathi S, et al. Text spotting transformers[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 9519-9528. 
                </li>
                <li id="textbpn++">
                    Zhang S X, Zhu X, Yang C, et al. Arbitrary shape text detection via boundary transformer[J]. arXiv preprint arXiv:2205.05320, 2022. 
                </li>
                <li id="pan">
                    Wang W, Xie E, Song X, et al. Efficient and accurate arbitrary-shaped text detection with pixel aggregation network[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2019: 8440-8449. 
                </li>
                
                <li id="CRNN">
                    Shi B, Bai X, Yao C. An end-to-end trainable neural network for image-based sequence recognition and its 
                    application to scene text recognition[J].
                    IEEE transactions on pattern analysis and machine intelligence.
                    2016, 39(11): 2298-2304.
                </li>
                <li id="TRBA">
                    Baek J, Kim G, Lee J, et al. What is wrong with scene text recognition model comparisons? dataset and model analysis[C]
                    Proceedings of the IEEE/CVF international conference on computer vision. 2019: 4715-4723.
                </li>
                <li id="ViTSTR">
                    Atienza R. Vision transformer for fast and efficient scene text recognition[C]
                    Document Analysis and Recognition–ICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 5–10, 2021, Proceedings, Part I 16. Springer International Publishing,
                     2021: 319-334.
                </li>
                <li id="SVTR">
                    Du Y, Chen Z, Jia C, et al. Svtr: Scene text recognition with a single visual model[J] 
                    arXiv preprint arXiv:2205.00159,2022.
                </li>
                <li id="SemiMTR">
                    Aberdam A, Ganz R, Mazor S, et al. Multimodal semi-supervised learning for text recognition[J]. 
                    arXiv preprint arXiv:2205.03873,2022.
                </li>
                <li id="PARSeq">
                    Bautista D, Atienza R. Scene text recognition with permuted autoregressive sequence models[C]
                    Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXVIII. Cham: Springer Nature Switzerland,
                    2022: 178-196.
                </li>
                <li id="MATRN">
                    Na B, Kim Y, Park S. Multi-modal text recognition networks: Interactive enhancements between visual and semantic features[C]
                    Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXVIII. Cham: Springer Nature Switzerland
                    2022: 446-463.
                   
                </li>
                <li id="ABINet">
                    Fang S, Xie H, Wang Y, et al. Read like humans: Autonomous, bidirectional and iterative language modeling for scene text recognition[C]
                    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
                     2021: 7098-7107.
                   
                </li>
                <li id="MASTER">
                    Lu N, Yu W, Qi X, et al. Master: Multi-aspect non-local network for scene text recognition[J]. Pattern Recognition.2021: 117: 107980.
                </li>
                <li id="Ours">
                   Todo
                </li>
            </ol>
        </div>
      </div> <!-- /container -->
    
</body>

<script>
    var myImage = document.getElementById("mypoly");

    myImage.addEventListener("mouseover", function() {
    this.src = "imgs/poly.jpg";
    });

    myImage.addEventListener("mouseout", function() {
    this.src = "imgs/poly_raw.jpg";
    });
</script>
</html>