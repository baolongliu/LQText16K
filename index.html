<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SHIP</title>
    <!-- <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-GLhlTQ8iRABdZLl6O3oVMWSktQOp6b7In1Zl3/Jr59b6EGGoI1aFkw7cmDA6j6gD" crossorigin="anonymous"> -->

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>

    <script type="text/javascript" src="js/showdown.min.js"></script>
    <style>
        * {
            /* font-family:  Century Gothic,CenturyGothic,AppleGothic,sans-serif;; */
            font-family: 'Times New Roman', Times, serif;
            /* font-style: oblique; */
            margin: 0%;
            padding: 0%;
        }

        html {
            scroll-behavior: smooth;
        }

        .container {
            text-align: center;
            width: 1000px;
            margin-left: auto;
            margin-right: auto;
        }

        .container>div {
            width: 1000px;
            display: inline-block;
            margin-top: 20px;
        }

        .title {
            font-size: 40px;
            font-weight: 400;
            margin-bottom: 10px;
        }

        p.authors {
            font-size: 20px;
            margin-bottom: 10px;
        }

        p.affi {
            padding: 0px;
            margin: 0px;
            font-size: 20px;
        }

        .model_img {
            margin-top: 20px;
            width: 900px;
        }

        span.title {
            display: block;
            text-align: left;
            padding-bottom: 5px;
            font-size: 30px;
            border-bottom: 1.5px solid rgb(229, 229, 229);
        }

        span.title_content {
            padding-top: 5px;
            display: block;
            text-align: justify;
            padding-bottom: 5px;
            font-size: 17px;
        }

        .performance_container {
            display: flex;
            justify-content: space-around;
        }

        span.dataset {
            display: block;
            font-size: 25px;
            padding-top: 20px;
            padding-bottom: 10px;
            font-weight: 400;
        }

        img.performance_img {
            width: 450px;
        }

        img.group_img {
            width: 1000px;
        }

        .sources_container {
            display: flex;
            justify-content: space-around;
            margin-bottom: 20px;
        }

        img.source_img {
            padding-top: 30px;
            display: block;
            width: 100px;
            transition: 0.5s;
        }

        img.source_img:hover {
            transform: scale(120%, 120%);
            -webkit-transform: scale(120%, 120%);
            -moz-transform: scale(120%, 120%);
        }

        span.source_title {
            display: block;
            margin-top: 15px;
            font-size: 25px;
        }

        .bib {
            text-align: left;
            font-weight: 400;
        }

        pre {
            text-align: left;
            display: block;
            padding: 9.5px;
            margin: 0 0 10px;
            font-size: 13px;
            line-height: 1.428571429;
            color: #333;
            word-break: break-all;
            word-wrap: break-word;
            background-color: #f5f5f5;
            border: 1px solid #ccc;
            border-radius: 4px;
        }

        table.dataset {
            margin: auto;
            border-collapse: collapse;
            border-top: 2px solid black;
            border-bottom: 2px solid black;
            margin-top: 2px;
        }

        tr.split {
            border-collapse: collapse;
            border-top: 1px solid black;
        }

        tr.model_name {
            font-size: 15px;
        }

        td {
            padding-top: 3px;
            padding-bottom: 3px;
            padding-left: 8px;
            padding-right: 8px;
        }

        th {
            padding-top: 3px;
            padding-bottom: 3px;
            padding-left: 8px;
            padding-right: 8px;
        }

        a {
            text-decoration: none;
        }

        a:visited,
        a:hover,
        a:link,
        a:active {
            color: #333;
        }

        ol.reference_content {
            margin-top: 10px;
            padding-left: 1.2em;
            text-align: justify;
            font-size: 15px;
            /* font-family:  Century Gothic,CenturyGothic,AppleGothic,sans-serif;; */
        }

        li {
            padding-bottom: 15px;
        }

        ul {
            margin-top: 10px;
            padding-left: 1.2em;
            text-align: justify;
            font-size: 17px;
        }

        .slider {
            position: relative;
            width: 100%;
            height: 400px;
            overflow: hidden;
        }

        .slider img {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            opacity: 0;
            transition: opacity 1s ease-in-out;
        }

        .slider img:first-child {
            opacity: 1;
        }

        .prev,
        .next {
            position: absolute;
            top: 50%;
            width: auto;
            margin-top: -22px;
            padding: 16px;
            color: white;
            font-weight: bold;
            font-size: 30px;
            transition: 0.6s ease;
            border-radius: 0 3px 3px 0;
            cursor: pointer;
            z-index: 1;
        }

        .next {
            right: 0;
            border-radius: 3px 0 0 3px;
        }

        .prev:hover,
        .next:hover {
            background-color: rgba(0, 0, 0, 0.8);
        }

        .image-container {
            display: flex;
            justify-content: space-between;
        }

        .image-wrapper {
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        .image-wrapper img {
            max-width: 100%;
        }

        .image-title {
            text-align: center;
        }
    </style>

</head>

<body>
    <!-- <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js" integrity="sha384-/mhDoLbDldZc3qpsJHpLogda//BVZbgYuw6kof4u2FrCedxOtgRZDTHgHUhOCVim" crossorigin="anonymous"></script> -->
    <div class="container">
        <div class="header">
            <h3 class="title">Going Deeper into Low-quality Text Recognition in
                the Wild: A New Benchmark Dataset</h3>
            <p class="authors">Baolong Liu<sup>1</sup> &nbsp;&nbsp;&nbsp;&nbsp;
                Ruiqing Yang<sup>1</sup> &nbsp;&nbsp;&nbsp;&nbsp;
                Wenhao Xu<sup>1</sup> &nbsp;&nbsp;&nbsp;&nbsp;
                Roukai Huang<sup>1</sup> &nbsp;&nbsp;&nbsp;&nbsp;
                Jianfeng Dong<sup>1</sup> &nbsp;&nbsp;&nbsp;&nbsp;
                Xun Wang<sup>1</sup>
            </p>
<!--             <p class="affi">¹School of Computer Science and Technology, Zhejiang Gongshang University</p> -->
            <!-- <p class="affi">²School of Information Science and Technology, University of Science and Technology of China</p> -->
            <!-- <p class="affi">³Key Lab of Data Engineering and Knowledge Engineering, Renmin University of China</p> -->
        </div>
        <div class="sources">
            <div class="modal fade" id="myModal" tabindex="-1" aria-labelledby="exampleModalLabel" aria-hidden="true">
                <div class="modal-dialog">
                    <div class="modal-content">
                        <!-- <div class="modal-header">
                            <h5 class="modal-title" id="exampleModalLabel">Tip</h5>
                            <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                        </div> -->
                        <div class="modal-body">
                            The dataset will be made public along with the paper.
                        </div>
                        <!-- <div class="modal-footer">
                            <button type="button" class="btn btn-secondary" data-bs-dismiss="modal">取消</button>
                            <button type="button" class="btn btn-primary">确定</button>
                        </div> -->
                    </div>
                </div>
            </div>
            
            <span class="title"></span>
            <div class="sources_container">
                <div class="source">
                    <a href="#" class="source_img" target="_blank">
                        <img src="imgs/paper.svg" alt="" class="source_img">
                    </a>
                    <span class="source_title">Paper</span>
                </div>
                <div class="source">
                    <a href="#" class="source_img" target="_blank">
                        <img src="imgs/ppt.svg" alt="" class="source_img">
                    </a>
                    <span class="source_title">Slides</span>
                </div>
                <div class="source">

                    <!-- <a href="#" data-bs-toggle="modal" data-bs-target="#myModal">
                        <img src="imgs/data.svg" alt="" class="source_img">
                    </a> -->
                    <a href="https://github.com/baolongliu/LQText16K" class="source_img" target="_blank">
                        <img src="imgs/data.svg" alt="" class="source_img">
                    </a>
                    <span class="source_title">Data</span>
                </div>
                <div class="source">
                    <a href="https://github.com/baolongliu/LQText16K" class="source_img" target="_blank">
                        <img src="imgs/code.svg" alt="" class="source_img">
                    </a>
                    <span class="source_title">Code</span>
                </div>
            </div>
            <div id="abs"></div>
            <!-- <pre>todo</pre> -->

        </div>
<!--         <div class="abstract">
            <span class="title">Abstract</span>
            <span class="title_content">
                We present a novel dataset, known as the Ship License Plate (SLP) dataset, comprising 16,000 ship images, each featuring at least one SLP. This dataset aims to facilitate natural scene text recognition tasks, including text detection, recognition, and end-to-end text recognition. Establishing this dataset was not without its challenges, such as the varied backgrounds, seasonal and lighting conditions, diverse character sets, layout, and ships. To overcome these obstacles, we implemented strict annotation guidelines and employed selective pairing methods to guarantee a dataset of exceptional quality and diversity.
            </span>
            <span class="title_content">
                The dataset features annotations that provide critical information, including the location, category, and transcription of each SLP instance. Our paper outlines the rigorous annotation process, which employed irregular quadrilaterals to denote the location of each SLP instance, seven distinct categories for SLP classification, and the character sequence of each SLP instance as training data for text recognition models.
            </span>
            <span class="title_content">
                In addition, our paper provides a comprehensive guide on selecting appropriate paired text instances to further enhance natural scene text recognition tasks. We construct three types of LQ-HQ text instances based on different text degradation types, including Clarity pairs, Spatial structure pairs, and Mixed degeneration pairs. Moreover, the dataset undergoes multi-level statistical analysis, including image-level, SLP instance-level, and pairing-level analysis. The resulting statistical data indicate that the dataset boasts exceptional diversity and representativeness, making it highly suitable for exploring natural scene text recognition tasks.
            </span>
            <span class="title_content">
                In summary, this dataset serves as a challenging and diverse benchmark for natural scene text recognition tasks, which can aid in the development and evaluation of natural scene text recognition algorithms.
            </span>
        </div> -->
        <div class="formulation">
            <span class="title">Introduction</span>
            <span class="title_content">
                This paper highlights the challenges and significance of recognizing low-quality text in natural scenes and proposes a method for capturing Chinese ship license plates to obtain low-quality and high-quality text pairs. The low-quality text in natural scenes is subject to two types of degradation: visual clarity and spatial structure degradation. These forms of degradation are particularly challenging for current text recognition techniques to address. Through the capture of Chinese ship license plates, we successfully obtained pairs of low-quality and high-quality license plates that are typically affected by various natural scene factors, such as weather and lighting. This makes them an excellent source of data for researching low-quality text recognition in natural scenes.
            </span>
            <span class="title_content">
                We have developed a benchmark dataset for recognizing low-quality text in natural scenes based on the data obtained from capturing Chinese ship license plates. This dataset encompasses various forms of low-quality and high-quality license plate pairs, including pairs with low-high visual clarity, low-high spatial structure, and mixed degradation low-high pairs. Furthermore, we propose a novel method for model training that employs MAE pre-training and permutation language models. Our approach yields state-of-the-art recognition outcomes on the benchmark dataset, demonstrating its efficacy in recognizing low-quality text in natural scenes.

            </span>
            <span class="title_content">
                In addition, we acknowledge the difficulties in obtaining pairs of low-quality and high-quality text in natural scenes. Such text pairs are typically not found together in a single scene, making their collection challenging. Nevertheless, we were able to overcome this obstacle by capturing Chinese ship license plates, which provided us with pairs of low-quality and high-quality license plates. These license plates are susceptible to various natural scene factors, including weather and lighting, making them an optimal data source for exploring low-quality text recognition in natural scenes.
            </span>
            <span class="title_content">

                Lastly, we outline the process of constructing our benchmark dataset for recognizing low-quality text in natural scenes, which includes details on its composition, labeling methods, and statistical analysis. Our dataset includes a diverse range of low-quality and high-quality license plate pairs, and has undergone multi-level statistical analysis to ensure its representativeness. As a result, this dataset is highly suitable for research on recognizing low-quality text in natural scenes.

            </span>
            <span class="title_content">
                The approach and dataset we have developed provide a rigorous and varied benchmark for recognizing low-quality text in natural scenes. This benchmark is essential for the development and evaluation of algorithms for recognizing low-quality text in natural scenes. We are confident that our work will offer valuable insights and guidance for future research in this field.
            </span>
            <br>
            <span class="title_content">
                You can click the arrows in the figure to view the sample images of the labelled images left and right.
            </span>
            <!-- <img id="mypoly" src="imgs/poly_raw.jpg" alt="" class="group_img"> -->
            <div>
                <div id="carouselExampleIndicators_annotion" class="carousel slide" style="margin: auto;">
                    <div class="carousel-inner">
                        
                        <div class="carousel-item active">
                            <img src="imgs/annotation/T_20211121_11_46_37_036915.jpg" class="d-block w-100" alt="...">
                            <!-- <div class="image-title">Clarity pairs</div> -->
                        </div>
                        <div class="carousel-item ">
                            <img src="imgs/annotation/T_20220704_14_32_04_978625.jpg" class="d-block w-100" alt="...">
                            <!-- <div class="image-title">Clarity pairs</div> -->
                        </div>
                        <div class="carousel-item ">
                            <img src="imgs/annotation/T_20220831_10_48_41_500000.jpg" class="d-block w-100" alt="...">
                            <!-- <div class="image-title">Clarity pairs</div> -->
                        </div>
                        <div class="carousel-item ">
                            <img src="imgs/annotation/T_20190213_12_10_52_156250.jpg" class="d-block w-100" alt="...">
                            <!-- <div class="image-title">Clarity pairs</div> -->
                        </div>

                    </div>
                    <button class="carousel-control-prev" type="button"
                        data-bs-target="#carouselExampleIndicators_annotion" data-bs-slide="prev">
                        <span class="carousel-control-prev-icon" aria-hidden="false" style="color: black;"></span>
                        <span class="visually-hidden">Previous</span>
                    </button>
                    <button class="carousel-control-next" type="button"
                        data-bs-target="#carouselExampleIndicators_annotion" data-bs-slide="next">
                        <span class="carousel-control-next-icon" aria-hidden="false" style="color: black;"></span>
                        <span class="visually-hidden" style="color: black;">Next</span>
                    </button>
                </div>
            </div>
           
        </div>
        <div class="method">
            <span class="title">Dataset Overview</span>
            <span class="title_content">
                <span class="title_content">When detecting and recognizing texts in different images, the main challenges can be summarized into two aspects: the complexities of the texts themselves (e.g., arbitrary character distributions, changeable plate positions, low resolutions and the character differences in backgrounds, color, size, typeface, language and quantity) and the variability of the external environments in which the texts appeared (e.g., different locations, different seasons, different illuminations). A good texts detection or recognition model should be able to cope with almost all the above challenges at the same time. </span>
                <span class="title_content"> Fig. 2(a) depicts the number of images collected from 8 different locations, from which one can see that the number of images of each location are about 2,000. The percentage of images collected from different seasons is as follows: spring 24.07%, summer 24.73%, autumn 26.07%, and winter 25.12%. These data suggest that the dataset proposed in this paper contains images with a relatively even distribution across seasons. The dataset in this paper contains 27.84% of images collected under night-time conditions, which makes it a useful dataset for studying text recognition in low-light environments. The images were collected over a period of five years, from July 2018 to November 2022, indicating that the dataset has a sufficient duration of image collection. </span>
                
                <!--                 <span class="title_content">
                    Fig. 2a illustrates the quantity of images amassed from eight disparate locations, revealing approximately 2,000 images per site. Owing to factors such as vessel frequency and imaging conditions, certain locations may exhibit a deviation from the 2,000-image benchmark (e.g., 2,762 images at location 5 and 1,539 images at location 6). Furthermore, to bolster the diversity of image backdrops, each location was photographed from a minimum of two distinct camera angles. Consequently, the comprehensive dataset comprises images from 21 unique vantage points across eight locations, guaranteeing a heterogeneous background for the proposed dataset.
                </span>
                <span class="title_content">
                    Per Fig. 2b, the proportional distribution of images accrued across seasons is as follows: spring 24.07%, summer 24.73%, autumn 26.07%, and winter 25.12%. These figures evince a relatively equitable seasonal allocation within the dataset.
                </span>
                <span class="title_content">
                    Fig. 2c delves into the number of images procured across years and months, revealing a collection period spanning five years, from July 2018 to November 2022. This denotes an ample duration for image acquisition.
                </span>
                <span class="title_content">Contrary to most extant datasets, which primarily encompass text instances gathered during daylight hours, Fig. 2d attests that 27.84% of the images within this dataset were obtained during nocturnal conditions, rendering it a valuable resource for examining text recognition in low-light settings. </span>
                <span class="title_content">
                    Through this image-level statistical analysis, the proposed dataset exhibits commendable diversity and representativeness regarding image backgrounds, seasonality, weather, months, and lighting conditions. This substantiates its aptitude for investigating natural scene text recognition tasks.
                </span> -->
                         
                        
            </span>
            <div>
                <div class="image-container">
                    <div class="image-wrapper">
                        <img src="imgs/number_per_pos.png" alt="Image 1">
                        <div class="image-title"> Fig. 2a Number of images in different locations</div>
                    </div>
                    <div class="image-wrapper">
                        <img src="imgs/number_per_season.png" alt="Image 2">
                        <div class="image-title"> Fig. 2b Number of images in different seasons</div>
                    </div>

                </div>
                <div class="image-container">
                    <div class="image-wrapper">
                        <img src="imgs/heatmap_diff_year_month.jpg" alt="Image 3">
                        <div class="image-title">Fig. 2c Distribution of the number of images in different years and months
                        </div>
                    </div>
                    <div class="image-wrapper">
                        <img src="imgs/day_or_night.png" alt="Image 4">
                        <div class="image-title">Fig. 2d Number of images during day and night</div>
                    </div>
                </div>
            </div>
        </div>

        <div class="method">
            <span class="title">Image Acquisition</span>
            <span class="title_content">
                <!-- This paper captured the images used in our work in Hangzhou City, Zhejiang Province, China. We aimed to ensure diversity in image backgrounds and to cover a wide range of imaging environments, including extreme situations. We captured images from eight different locations of three different rivers: the Beijing-Hangzhou Grand Canal, the Qiantang River, and the Hangzhou-Ningbo Canal, with different viewpoints. We used the Hikvision iDS-2CD9371-KS seven-megapixel intelligent traffic network camera at each site to capture images of ships. We sampled images randomly from the eight locations. -->

                Hangzhou, the southern terminus of the Beijing-Hangzhou Grand Canal, has a well-developed waterway
                transportation system with numerous ships passing through day and night. All the images utilized in this
                work were acquired in Hangzhou City, Zhejiang Province, China to ensure diversity in image backgrounds.
                The images were captured from eight different locations of three different rivers: the Beijing-Hangzhou
                Grand Canal, the Qiantang River, and the Hangzhou-Ningbo Canal, with different viewpoints. The imaging
                quality of cameras in the wild may differ greatly depending on the season, day, and weather, and to
                cover a wide range of imaging environments, especially in extreme situations, the time span of image
                capturing lasted four years. The Hikvision iDS-2CD9371-KS seven-megapixel intelligent traffic network
                camera was used at each site to capture images of ships.
<!--                 We randomly sampled an image from 8 locations, and the equipment used for the capture was the Hikvision
                iDS-2CD9371-KS seven megapixel intelligent traffic network camera. -->
            </span>
            <br>
            <span class="title_content">
                You can click the arrows in the figure below to view the sample images of our collected  data left and right.
            </span>
            <div>
                <div id="carouselExampleIndicators_preview" class="carousel slide" style="margin: auto;">
                    <div class="carousel-inner">
                        <div class="carousel-item active">
                            <img src="imgs/O_20220102_16_41_48_437250.jpg" class="d-block w-100" alt="...">
                            <!-- <div class="image-title">Clarity pairs</div> -->
                        </div>
                        <div class="carousel-item ">
                            <img src="imgs/O_20220107_23_36_26_171875.jpg" class="d-block w-100" alt="...">
                            <!-- <div class="image-title">Clarity pairs</div> -->
                        </div>
                        <div class="carousel-item ">
                            <img src="imgs/O_20220108_10_48_03_066875.jpg" class="d-block w-100" alt="...">
                            <!-- <div class="image-title">Clarity pairs</div> -->
                        </div>
                        <div class="carousel-item ">
                            <img src="imgs/O_20180706_12_01_09_765625.jpg" class="d-block w-100" alt="...">
                            <!-- <div class="image-title">Clarity pairs</div> -->
                        </div>
                        <div class="carousel-item ">
                            <img src="imgs/O_20220519_05_19_15_911475.jpg" class="d-block w-100" alt="...">
                            <!-- <div class="image-title">Clarity pairs</div> -->
                        </div>
                        <div class="carousel-item ">
                            <img src="imgs/T_20211201_06_53_48_446553.jpg" class="d-block w-100" alt="...">
                            <!-- <div class="image-title">Clarity pairs</div> -->
                        </div>
                        <div class="carousel-item ">
                            <img src="imgs/T_20220601_08_05_10_921875.jpg" class="d-block w-100" alt="...">
                            <!-- <div class="image-title">Clarity pairs</div> -->
                        </div>
                        <div class="carousel-item ">
                            <img src="imgs/O_20210816_14_05_41_187500.jpg" class="d-block w-100" alt="...">
                            <!-- <div class="image-title">Clarity pairs</div> -->
                        </div>

                    </div>
                    <button class="carousel-control-prev" type="button"
                        data-bs-target="#carouselExampleIndicators_preview" data-bs-slide="prev">
                        <span class="carousel-control-prev-icon" aria-hidden="false" style="color: black;"></span>
                        <span class="visually-hidden">Previous</span>
                    </button>
                    <button class="carousel-control-next" type="button"
                        data-bs-target="#carouselExampleIndicators_preview" data-bs-slide="next">
                        <span class="carousel-control-next-icon" aria-hidden="false" style="color: black;"></span>
                        <span class="visually-hidden" style="color: black;">Next</span>
                    </button>
                </div>
            </div>
        </div>

        <div class="method">
            <span class="title">Image Annotation</span>
            <span class="title_content">
                        Considering the aforementioned factors, we distill our primary annotation guidelines as follows:
            <span class="title_content">
                1) Semantic consistency rule: Despite potentially disordered visual character arrangements in SLPs, the ground truth reading order should adhere to the corresponding official laws or regulations. Any deviation may yield illegitimate and nonsensical SLPs. Thus, regardless of on-site visual arrangements, as exhibited in Fig. 3b, Fig. 3c, and Fig. 3e, we transcribe characters by honoring their semantic ground truth.
            <span class="title_content">
                2) Associative compensation rule: Many SLPs appear incomplete due to complex imaging environments, obscurity, vagueness, or corrosion. Nonetheless, SLPs often possess ample semantic clues. Furthermore, a ship may display multiple SLPs with identical meanings, some incomplete while others are clear and complete. Consequently, humans can infer missing characters by leveraging semantic clues or complete SLPs. We expect algorithms, machines, or architectures to surmise missing information by learning from other semantic cues.
            </span>
            <span class="title_content">
                3) Port category: As port information is a crucial identification aspect, we designate the port name as a separate class. Often printed vertically, as in Fig. 3d, we apply the semantic consistency rule to transcribe Chinese characters and their subsequent Pinyin.
            </span>
            <span class="title_content">
                4) Reverse order: Some SLPs are printed in reverse order, deviating from conventional reading habits. For these instances, we categorize them separately and transcribe them based on the semantic consistency rule.
            </span>
            <span class="title_content">
                 5) Separated SLP: Certain SLPs are written as multiple sub-parts with considerable distances between them. As illustrated in Fig. 3e, we classify these SLPs as a distinct category and transcribe them as a single, complete SLP using the semantic consistency rule.
            </span>
            <span class="title_content">
                6) Visibility category: For incomplete SLPs, as demonstrated in Fig. 3g, we classify them based on visibility, i.e., the percentage of illegible characters within the SLP. If the associative compensation rule enables us to compensate for illegible characters, we do so; otherwise, we categorize the SLP as unreadable and leave it untranscribed.
            </span>
            
            <span class="title_content">
                To ensure exceptional annotation quality, we enlisted 26 graduate students and 2 teachers with expertise in artificial intelligence. Possessing robust backgrounds in object detection, text recognition, and computer vision, these individuals underwent comprehensive training prior to official commencement. Teachers functioned as inspectors to guarantee the quality of annotations. The entire annotation process spanned approximately six months.
            </span>
            
           
            
            


            </span>
            <div class="image-wrapper">
                <img src="imgs/annotation.png" alt="...">
                <div class="image-title"> Fig. 3 Examples of the seven categories of ship license plates and their labeling rule</div>
            </div>
        </div>
        <!-- <div class="method">
            <span class="title">Paired Data Visualization</span>
            <div class="image-wrapper">
                <img src="imgs/PPT_visual_new2.png" alt="...">
                <div class="image-title">Clarity pairs</div>
            </div>
            <div class="image-wrapper">
                <img src="imgs/PPT_space_new3.png" alt="...">
                <div class="image-title">Spatial structure pairs</div>
            </div>
            <div class="image-wrapper">
                <img src="imgs/PPT_space_visual.png" alt="...">
                <div class="image-title">Mixed degeneration pairs</div>
            </div>

        </div> -->
        <div class="method">
            <span class="title">Paired Data Visualization</span>
            <span class="title_content">
                1) Clarity pairs: In order to enable the algorithm to restore
                the clarity and recognition of the LQ text, we manually
                selected a subset of text pairs containing identical character
                sequences of HQ clear text and LQ degraded text. Specifically,
                we first select a character-clear and unambiguous HQ SLP
                image from a certain ship, and then search an LQ SLP image
                from the same ship that contained the same character sequence
                to form an LQ-HQ text pair. the searched clarity degradation SLP
                image may contain blurry, occluded,and abnormal lighting conditions that cause abnormal charac-
                ter gray-scale values.
            </span>

            <span class="title_content">
                2) Spatial structure pairs: Another major type of degradation for LQ text is spatial structure degradation. Therefore,
                we have also selected different forms of spatial structure LQ-HQ text pairs with a specific aim. The selection of these
                image pairs was also performed manually. In detail, we first
                select an HQ SLP with relatively regular character distribution
                that is easy for existing algorithms to recognize. Then, we
                search for an LQ SLP with the same character sequence but
                different character spatial structures.

            </span>
            <span class="title_content">

                3) Mixed degeneration pairs: Taking into account the hybrid nature of text degradation, many LQ texts in reality
                not only suffer from degradation in terms of clarity but also
                from degradation in terms of spatial structure. Therefore,
                constructing text image pairs that contain various types of
                degradation is also very necessary. Meanwhile, in order to
                expand the number of text image pairs, we further employ an
                image similarity calculation algorithm, SSIM [36], to select a
                larger scale of LQ-HQ text image pairs. The SSIM algorithm is
                a popular method for calculating the similarity of image pairs.
                It considers the similarity of images in terms of brightness,
                contrast, and structure simultaneously. Based on the algorithm-based confidence of image similarity, we not only expand the
                number of clarity-type and spatial structure-type image pairs,
                but also obtained LQ-HQ image pairs that simultaneously
                contain clarity and spatial structure differences
            </span>
            <br>
            <span class="title_content">
                You can click the arrows in the figure below to view the sample images of our  paired  data left and right.
            </span>
            <div>
                <div id="carouselExampleIndicators" class="carousel slide" style="margin: auto;">
                    <div class="carousel-inner">
                        <div class="carousel-item active">
                            <img src="imgs/PPT_visual_new2.png" class="d-block w-100" alt="...">
                            <div class="image-title">Clarity pairs</div>
                        </div>
                        <div class="carousel-item">
                            <img src="imgs/PPT_space_new3.png" class="d-block w-100" alt="...">
                            <div class="image-title">Spatial structure pairs</div>
                        </div>
                        <div class="carousel-item">
                            <img src="imgs/PPT_space_visual.png" class="d-block w-100" alt="...">
                            <div class="image-title">Mixed degeneration pairs</div>
                        </div>
                    </div>
                    <button class="carousel-control-prev" type="button" data-bs-target="#carouselExampleIndicators"
                        data-bs-slide="prev">
                        <span class="carousel-control-prev-icon" aria-hidden="false" style="color: black;"></span>
                        <span class="visually-hidden">Previous</span>
                    </button>
                    <button class="carousel-control-next" type="button" data-bs-target="#carouselExampleIndicators"
                        data-bs-slide="next">
                        <span class="carousel-control-next-icon" aria-hidden="false" style="color: black;"></span>
                        <span class="visually-hidden" style="color: black;">Next</span>
                    </button>
                </div>
            </div>
        </div>
<!-- 

        <div class="method">
            <span class="title">Paired Data Vs. TextZoom</span>
            <div>
                <div class="image-container">
                    <div class="image-wrapper">
                        <img src="imgs/TextZoom_vis.png" alt="Image 1">
                        <div class="image-title">TextZoom</div>
                    </div>


                </div>
                <div class="image-container">
                    <div class="image-wrapper">
                        <img src="imgs/PPT.png" alt="Image 2">
                        <div class="image-title">Our Paired Data</div>
                    </div>
                </div>
            </div>
        </div> -->


        <div class="method">
            <span class="title">Dataset Statistics</span>
            <!-- <span class="title_content">
                        The SLP instance-level statis-
        tical analyses are also conducted in Fig. 4. We divided the
        collected into five categories based on their character spatial
        distribution patterns, namely single line, two lines, multiple
        lines, segmented, and vertical. The number of SLPs in each
        category is shown in Fig. 4d. Fig. 4d shows that the two-line
        SLPs are the most and the segmented SLPs are the fewest, but
        there are at least 2,000 instances of each type of SLP in the
        dataset overall. Regarding the scale of SLP, we respectively
        conducted statistical analysis on SLP width and height. From
        Fig. 4a, it can be seen that the majority of SLPs have widths
        below 400 pixels, with the percentage of widths less than 100
        pixels being largest. However, there are also some SLPs with
        widths greater than 400 pixels. In terms of height, the majorit 
        of SLPs have heights between 30 and 150 pixels, while there
        are also some SLPs with very large height values (greater
        than 150) or very small height values (less than 30). It can be
        observed that the distribution of scales of SLP text instances
        has good diversity, indicating that the dataset is a challenging
        text recognition dataset containing various aspect ratios of
        instances. However, the large scale diversity also corresponds
        to the distribution of text scales in real-world environments.
        We also conducted some statistical analysis on the characters
        included in SLPs. Fig. 4c displays the statistical distribution
        of the number of characters contained in different SLPs. It
        can be observed that the number of characters contained in
        SLPs ranges from 2 to 40, and the number of SLPs containing
        8 characters is the highest. Fig. 4e summarizes the top 50
        characters contained in SLPs, showing that SLPs contain not
        only Chinese characters but also English and digits, indicating
        a rich character diversity of the proposed dataset. Fig. 4f shows
        a statistical graph of the ports to which the ships in this dataset
        belong. It can be seen that the ships in this dataset came from
        multiple ports in different cities and provinces in China, with
        over 100 ships belonging to 10 of these ports. Based on the
        statistics in Fig. 4, it can be seen that the proposed dataset
        is very challenging and diverse at the instance level, with a
        wide range of instance scales, types, and character lengths and
        types
            </span> -->

            <span class="title_content">
            The proffered dataset presents a formidable and multifarious collection, encompassing an extensive assortment of instance scales, classifications, and character dimensions and varieties. SLP instances are apportioned into quintuple categories predicated upon their character spatial dissemination patterns. A preponderance of SLPs possess widths beneath 400 pixels, whilst the majority exhibit heights between 30 and 150 pixels. SLPs' character count extends from 2 to 40, and the uppermost 50 characters within SLPs incorporate not solely Chinese characters but English and numerals as well. Vessels in this dataset originate from numerous ports spanning diverse cities and provinces throughout China
            </span>
            
            <div>
                <div class="image-container">
                    <div class="image-wrapper">
                        <img src="imgs/width.png" alt="Image 1">
                        <div class="image-title">(a)</div>
                    </div>
                    <div class="image-wrapper">
                        <img src="imgs/height.png" alt="Image 1">
                        <div class="image-title">(b)</div>
                    </div>

                </div>
                <div class="image-container">
                    <div class="image-wrapper">
                        <img src="imgs/char_len.png" alt="Image 2">
                        <div class="image-title">(c)</div>
                    </div>
                    <div class="image-wrapper">
                        <img src="imgs/type_count.png" alt="Image 2">
                        <div class="image-title">(d)</div>
                    </div>
                </div>
                <div class="image-container">
                    <div class="image-wrapper">
                        <img src="imgs/char_top50.png" alt="Image 2">
                        <div class="image-title">top 50 characters in instance</div>
                        <div class="image-title">(e)</div>
                    </div>
                </div>
                <div class="image-container">
                    <div class="image-wrapper">
                        <img src="imgs/number_of_ship_per_port.png" alt="Image 2">
                        <div class="image-title">number of ship per port</div>
                        <div class="image-title">(f)</div>
                    </div>
                </div>
            </div>
        </div>


        <div class="performance">
            <span class="title">Baseline Algorithms</span>

            <div class="datasets">
                <span class="title_content">
                    When selecting SLP detection
                    algorithms, we considered multiple factors including performance, generalization, and stability. Cutting-edge text detection algorithms were estimated on our SLP dataset.
                </span>
                <span class="dataset">Text Detection Baselines</span>

                <table class="dataset" style="width: 500px;">
                    <tr>
                        <th colspan="5" align="left">Model</th>
                        <th>Year</th>
                        <th>Publisher</th>
                        <th>Precision</th>
                        <th>Recall</th>
                        <th>F-Measure</th>
                    </tr>
                    <!-- <tr class="split">
                        <td align="left"><em>T2VR models:</em></td>
                    </tr> -->
                    <tr class="model_name split">
                        <td colspan="5" align="left">DB++ <a href="#db++">[1]</a></td>
                        <td>2022</td>
                        <td>TPAMI</td>
                        <td>91.63</td>
                        <td>35.30</td>
                        <td>50.97</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left">I3CL <a href="#i3cl">[2]</a></td>
                        <td>2021</td>
                        <td>IJCV</td>
                        <td>49.70</td>
                        <td>60.60</td>
                        <td>54.66</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left">EAST <a href="#east">[3]</a></td>
                        <td>2017</td>
                        <td>CVPR</td>
                        <td>80.21</td>
                        <td>49.37</td>
                        <td>61.30</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left">FCENet <a href="#fcenet">[4]</a></td>
                        <td>2021</td>
                        <td>CVPR</td>
                        <td>80.92</td>
                        <td>50.82</td>
                        <td>62.43</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left">DPText-DETR <a href="#dptext-detr">[5]</a></td>
                        <td>2023</td>
                        <td>AAAI</td>
                        <td>69.53</td>
                        <td>72.17</td>
                        <td>70.82</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left">TESTR <a href="#testr">[6]</a></td>
                        <td>2022</td>
                        <td>CVPR</td>
                        <td>80.09</td>
                        <td>76.83</td>
                        <td>78.43</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left">TextBPN++ <a href="#textbpn++">[7]</a></td>
                        <td>2022</td>
                        <td>TPAMI</td>
                        <td>88.25</td>
                        <td>76.70</td>
                        <td>82.07</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left">PAN <a href="#pan">[8]</a></td>
                        <td>2019</td>
                        <td>ICCV</td>
                        <td>91.76</td>
                        <td>88.20</td>
                        <td>89.94</td>
                    </tr>
<!--                     <tr class="model_name">
                        <td colspan="5" align="left">YOLOv5+CSL </td>
                        <td> </td>
                        <td> </td>
                        <td>88.63</td>
                        <td>86.58</td>
                        <td>90.77</td>
                    </tr> -->
<!--                     <tr class="model_name">
                        <td colspan="5" align="left">YOLOv8 </td>
                        <td> </td>
                        <td> </td>
                        <td>90.93</td>
                        <td>93.83</td>
                        <td>92.14</td>
                    </tr> -->
                </table>
            </div>
            <br>

            </br>
            <div class="datasets">
                <span class="title_content">
<!--                     For recognition, we uniformly resized the images to a size of 224*224, with 3 channels for the
                    image. The encoder and decoder have a dimension of 768, and there are a total of 559 characters,
                    including English, Chinese, numbers, and common punctuation marks. The max label length is set to
                    50, and the overall architecture uses an encoder-decoder framework, which is divided into two stages
                    for training. In the first stage, we used a pre-training encoder scheme to train a more outstanding
                    encoder. Our encoder is inspired by the mask auto-encoder, which trains the encoder in the form of
                    image restoration. In the second stage, we combined the pre-trained encoder with Permutation
                    Language Model to connect with our downstream tasks, which is competitive in terms of convergence
                    speed and inference speed, and the accuracy reaches State-Of-The-Art in our datasets. -->
                    State-of-the-art text recognition algorithms were evaluated on our proposed SLP dataset. We also present a new baseline SLP recognition algorithm, which obtains state-of-the-art recognition accuracy. 
                </span>
                <span class="dataset">Text Recognition Baselines</span>
                <table class="dataset" style="width: 600px;">
                    <tr>
                        <th colspan="6" align="left">Method</th>
                        <th>Year</th>
                        <th>Publisher</th>
                        <th>LM</th>
                        <th>Params(M)</th>
                        <th>FPS</th>
                        <th>NED</th>
                        <th>Accuary</th>
                    </tr>
                    <tr class="split">
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">CRNN <a href="#CRNN">[9]</a></td>
                        <td>2016</td>
                        <td>TPAMI</td>
                        <td>-</td>
                        <td>8.59</td>
                        <td>144.93</td>
                        <td>43.64</td>
                        <td>5.29</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">TRBA <a href="#TRBA">[10]</a></td>
                        <td>2019</td>
                        <td>ICCV</td>
                        <td>-</td>
                        <td>33.65</td>
                        <td>44.43</td>
                        <td>64.76</td>
                        <td>30.26</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">ViTSTR <a href="#ViTSTR">[11]</a></td>
                        <td>2021</td>
                        <td>ICDAR</td>
                        <td>-</td>
                        <td>85.83</td>
                        <td>83.33</td>
                        <td>76.67</td>
                        <td>44.60</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">SVTR <a href="#SVTR">[12]</a></td>
                        <td>2022</td>
                        <td>IJCAI</td>
                        <td>-</td>
                        <td>22.33</td>
                        <td>69.50</td>
                        <td>80.23</td>
                        <td>45.16</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">SemiMTR <a href="#SemiMTR">[13]</a></td>
                        <td>2022</td>
                        <td>-</td>
                        <td>&#10003;</td>
                        <td>37.81</td>
                        <td>5.92</td>
                        <td>75.13</td>
                        <td>49.76</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">PARSeq <a href="#PARSeq">[14]</a></td>
                        <td>2022</td>
                        <td>ECCV</td>
                        <td>-</td>
                        <td>24.20</td>
                        <td>34.13</td>
                        <td>77.86</td>
                        <td>56.52</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">MATRN<a href="#MATRN">[15]</a></td>
                        <td>2022</td>
                        <td>ECCV</td>
                        <td>&#10003;</td>
                        <td>45.77</td>
                        <td>16.93</td>
                        <td>79.74</td>
                        <td>59.39</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">ABINet<a href="#ABINet">[16]</a></td>
                        <td>2021</td>
                        <td>CVPR</td>
                        <td>&#10003;</td>
                        <td>37.81</td>
                        <td>24.14</td>
                        <td>80.28</td>
                        <td>59.71</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">MASTER <a href="#MASTER">[17]</a></td>
                        <td>2021</td>
                        <td>PR</td>
                        <td>-</td>
                        <td>69.86</td>
                        <td>7.86</td>
                        <td>80.53</td>
                        <td>63.17</td>
                    </tr>
                    <tr class="model_name split">
                        <td colspan="6" align="left">Ours<a href="#Ours"></a></td>
                        <td>2023</td>
                        <td>-</td>
                        <td>-</td>
                        <td>122</td>
                        <td>34.41</td>
                        <td>85.92</td>
                        <td>66.01</td>
                    </tr>
                </table>
            </div>
        </div>
        <!-- <div class="group">
            <span class="title">Algorithm Visualization</span>
            <span class="title_content">

            </span>
            <img id="mypoly" src="imgs/poly_raw.jpg" alt="" class="group_img">
            <span class="title_content">

            </span>
        </div> -->
        <div class="acknowledgement">
            <span class="title">Acknowledgement</span>
            <span class="title_content">
                This work was supported by the National
                Key R&D Program of China (2018YFB1404102), NSFC (62172420,
                61902347, 61976188, 62002323), the Public Welfare Technology Research Project of Zhejiang Province
                (LGF21F020010), the Open
                Projects Program of the National Laboratory of Pattern Recognition,
                the Fundamental Research Funds for the Provincial Universities of
                Zhejiang, and Public Computing Cloud of RUC.
            </span>
        </div>

        <div class="contact">

            <span class="title">Contributors and Contact Information of the Dataset</span>

            <!-- <div id="result_md"></div>
             -->

            <span class="title_content">
                Below are the creators and maintainers of our dataset. Please feel free to contact us for more
                information:
            </span>
            <span class="title_content">
                <li>Creator: <b><em>School of Computer and Information Engineering, Zhejiang Gongshang University</em>
                </li></b>
                <li>Maintainer: <b><em>School of Computer and Information Engineering, Zhejiang Gongshang
                            University</em></li></b>
            </span>



            <span class="title_content">
                We welcome your feedback and opinions and will reply to you as soon as possible. If you encounter common
                problems with the dataset, please refer to the following questions and answers:
            </span>
            <span class="title_content">
                Question 1: What is the size and dimension of the dataset?
            </span>
            <span class="title_content">
                <span style="padding-left: 40px;">Answer: The dataset contains 16000 samples, each with at least one
                    attribute</span>
            </span>
            <span class="title_content">
                Question 2: What is the authorization and license of the dataset?
            </span>
            <span class="title_content">
                <span style="padding-left: 40px;">Answer: The authorization and license of the dataset is
                    CDLA-Sharing-1.0. Please see our authorization and license page for more information.</span>
            </span>
            <span class="title_content">
                Question 3: What are the download and access methods of the dataset?
            </span>
            <span class="title_content">
                <span style="padding-left: 40px;">Answer: You can download and access the dataset through the following
                    methods:</span>
                <span class="title_content" style="padding-left: 100px;"> Method 1: Visit our dataset page and click the
                    download button.</span>
                <span class="title_content" style="padding-left: 100px;"> Method 2: Contact our data science team and
                    request access to the dataset</span>
            </span>


            <span class="title_content">
                Question 4: What are the usage examples and application scenarios of the dataset?
            </span>
            <span class="title_content">
                <span style="padding-left: 40px;">Answer: The dataset can be used in the following application
                    scenarios:</span>
                <span class="title_content" style="padding-left: 100px;"> Scenario 1: Scientific research</span>
                <span class="title_content" style="padding-left: 100px;"> Scenario 2: Verify OCR model
                    performance</span>
            </span>

            <span class="title_content">
                Question 5: What are the limitations and warnings of the dataset?
            </span>
            <span class="title_content">
                <span style="padding-left: 40px;">Answer: The limitations and warnings of the dataset is commercial use.
                    Please see our dataset limitations and warnings page for more information.</span>
            </span>
            <span class="title_content">
                If you have any questions or suggestions regarding the dataset, please email us at the following
                address:
            </span>
            <ul>
                <li>Baolong Liu: <b><em>liubaolongx@gmail.com</em></li></b>
                <li>Ruiqing Yang: <b><em>yrq991121@gmail.com</em></li></b>
            </ul>
        </div>
        <div class="reference">
            <span class="title">Reference</span>
            <ol class="reference_content">
                <li id="db++">
                    Liao M, Zou Z, Wan Z, et al. Real-time scene text detection with differentiable binarization and
                    adaptive scale fusion[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022,
                    45(1): 919-931.
                </li>
                <li id="i3cl">
                    Du B, Ye J, Zhang J, et al. I3cl: intra-and inter-instance collaborative learning for
                    arbitrary-shaped scene text detection[J]. International Journal of Computer Vision, 2022, 130(8):
                    1961-1977.
                </li>
                <li id="east">
                    Zhou X, Yao C, Wen H, et al. East: an efficient and accurate scene text detector[C]//Proceedings of
                    the IEEE conference on Computer Vision and Pattern Recognition. 2017: 5551-5560.
                </li>
                <li id="fcenet">
                    Zhu Y, Chen J, Liang L, et al. Fourier contour embedding for arbitrary-shaped text
                    detection[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.
                    2021: 3123-3131.
                </li>
                <li id="dptext-detr">
                    Ye M, Zhang J, Zhao S, et al. Dptext-detr: Towards better scene text detection with dynamic points
                    in transformer[J]. arXiv preprint arXiv:2207.04491, 2022.
                </li>
                <li id="testr">
                    Zhang X, Su Y, Tripathi S, et al. Text spotting transformers[C]//Proceedings of the IEEE/CVF
                    Conference on Computer Vision and Pattern Recognition. 2022: 9519-9528.
                </li>
                <li id="textbpn++">
                    Zhang S X, Zhu X, Yang C, et al. Arbitrary shape text detection via boundary transformer[J]. arXiv
                    preprint arXiv:2205.05320, 2022.
                </li>
                <li id="pan">
                    Wang W, Xie E, Song X, et al. Efficient and accurate arbitrary-shaped text detection with pixel
                    aggregation network[C]//Proceedings of the IEEE/CVF international conference on computer vision.
                    2019: 8440-8449.
                </li>

                <li id="CRNN">
                    Shi B, Bai X, Yao C. An end-to-end trainable neural network for image-based sequence recognition and
                    its
                    application to scene text recognition[J].
                    IEEE transactions on pattern analysis and machine intelligence.
                    2016, 39(11): 2298-2304.
                </li>
                <li id="TRBA">
                    Baek J, Kim G, Lee J, et al. What is wrong with scene text recognition model comparisons? dataset
                    and model analysis[C]
                    Proceedings of the IEEE/CVF international conference on computer vision. 2019: 4715-4723.
                </li>
                <li id="ViTSTR">
                    Atienza R. Vision transformer for fast and efficient scene text recognition[C]
                    Document Analysis and Recognition–ICDAR 2021: 16th International Conference, Lausanne, Switzerland,
                    September 5–10, 2021, Proceedings, Part I 16. Springer International Publishing,
                    2021: 319-334.
                </li>
                <li id="SVTR">
                    Du Y, Chen Z, Jia C, et al. Svtr: Scene text recognition with a single visual model[J]
                    arXiv preprint arXiv:2205.00159,2022.
                </li>
                <li id="SemiMTR">
                    Aberdam A, Ganz R, Mazor S, et al. Multimodal semi-supervised learning for text recognition[J].
                    arXiv preprint arXiv:2205.03873,2022.
                </li>
                <li id="PARSeq">
                    Bautista D, Atienza R. Scene text recognition with permuted autoregressive sequence models[C]
                    Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
                    Proceedings, Part XXVIII. Cham: Springer Nature Switzerland,
                    2022: 178-196.
                </li>
                <li id="MATRN">
                    Na B, Kim Y, Park S. Multi-modal text recognition networks: Interactive enhancements between visual
                    and semantic features[C]
                    Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
                    Proceedings, Part XXVIII. Cham: Springer Nature Switzerland
                    2022: 446-463.

                </li>
                <li id="ABINet">
                    Fang S, Xie H, Wang Y, et al. Read like humans: Autonomous, bidirectional and iterative language
                    modeling for scene text recognition[C]
                    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
                    2021: 7098-7107.

                </li>
                <li id="MASTER">
                    Lu N, Yu W, Qi X, et al. Master: Multi-aspect non-local network for scene text recognition[J].
                    Pattern Recognition.2021: 117: 107980.
                </li>
<!--                 <li id="Ours">
                    Todo
                </li> -->
            </ol>
        </div>
    </div> <!-- /container -->

</body>

<script>

    //  window.onload = function(){
    //     //获取要转换的文字
    // text = `
    // Contributors and Contact Information of the Dataset

    // Below are the creators and maintainers of our dataset. Please feel free to contact us for more information:

    // - Creator: XXX Technology Co., Ltd.
    // - Maintainer: Data Science Team of XXX Technology Co., Ltd.

    // If you have any questions or suggestions regarding the dataset, please email us at the following address:

    // - Email: [data@xxx.com](mailto:data@xxx.com)

    // We welcome your feedback and opinions and will reply to you as soon as possible. If you encounter common problems with the dataset, please refer to the following questions and answers:

    // - Question 1: What is the size and dimension of the dataset?
    //     - Answer: The dataset contains XXXX samples, each with XXXX attributes.
    // - Question 2: What is the authorization and license of the dataset?
    //     - Answer: The authorization and license of the dataset is XXXX. Please see our authorization and license page for more information.
    // - Question 3: What are the download and access methods of the dataset?
    //     - Answer: You can download and access the dataset through the following methods:
    //         - Method 1: Visit our dataset page and click the download button.
    //         - Method 2: Contact our data science team and request access to the dataset.
    // - Question 4: What are the usage examples and application scenarios of the dataset?
    //     - Answer: The dataset can be used in the following application scenarios:
    //         - Scenario 1: XXX
    //         - Scenario 2: XXX
    // - Question 5: What are the limitations and warnings of the dataset?
    //     - Answer: The limitations and warnings of the dataset are XXXX. Please see our dataset limitations and warnings page for more information.
    //     `
    // //创建实例
    // var converter = new showdown.Converter();
    // //进行转换
    // var html = converter.makeHtml(text);
    // //展示到对应的地方  result便是id名称
    // document.getElementById("result_md").innerHTML = html;
    // console.log("aaaaaaa")
    // }

    // const alertPlaceholder = document.getElementById('liveAlertPlaceholder')

    // const alert = (message, type) => {
    //     const wrapper = document.createElement('div')
    //     wrapper.innerHTML = [
    //         `<div class="alert alert-${type} alert-dismissible" role="alert">`,
    //         `   <div>${message}</div>`,
    //         '   <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>',
    //         '</div>'
    //     ].join('')

    //     alertPlaceholder.append(wrapper)
    // }

    // const alertTrigger = document.getElementById('liveAlertBtn')
    // if (alertTrigger) {
    //     alertTrigger.addEventListener('click', () => {
    //         alert('Nice, you triggered this alert message!', 'success')
    //     })
    // }
    // var myModal = new bootstrap.Modal(document.getElementById('myModal'), {
    //     keyboard: false
    // })

    // var confirmBtn = document.querySelector('#myModal .modal-footer .btn-primary')
    // confirmBtn.addEventListener('click', function () {
    //     // 执行相应的操作
    //     myModal.hide()
    // })

    var myImage = document.getElementById("mypoly");

    myImage.addEventListener("mouseover", function () {
        this.src = "imgs/poly.jpg";
    });

    myImage.addEventListener("mouseout", function () {
        this.src = "imgs/poly_raw.jpg";
    });


    var slideIndex = 1;
    showSlides(slideIndex);

    function plusSlides(n) {
        showSlides(slideIndex += n);
    }

    function showSlides(n) {
        var i;
        var slides = document.getElementsByClassName("slider")[0].getElementsByTagName("img");
        if (n > slides.length) {
            slideIndex = 1
        }
        if (n < 1) {
            slideIndex = slides.length
        }
        for (i = 0; i < slides.length; i++) {
            slides[i].style.opacity = 0;
        }
        slides[slideIndex - 1].style.opacity = 1;
    }

</script>

</html>
