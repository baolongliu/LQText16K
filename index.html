
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SHIP</title>
    <style>
        * {
            /* font-family:  Century Gothic,CenturyGothic,AppleGothic,sans-serif;; */
            font-family: 'Times New Roman', Times, serif;
            /* font-style: oblique; */
            margin: 0%;
            padding: 0%;
        }
        html {
            scroll-behavior: smooth;
        }

        .container {
            text-align: center;
            width: 1000px;
            margin-left:auto;  
            margin-right:auto;
        }
        .container > div {
            width: 1000px;
            display: inline-block;
            margin-top: 20px;
        }
        .title {
            font-size: 40px;
            font-weight: 400;
            margin-bottom: 10px;
        }
        p.authors {
            font-size: 20px;
            margin-bottom: 10px;
        }
        p.affi {
            padding: 0px;
            margin: 0px;
            font-size: 20px;
        }
        .model_img {
            margin-top: 20px;
            width: 900px;
        }
        span.title {
            display: block;
            text-align: left;
            padding-bottom: 5px;
            font-size: 30px;
            border-bottom: 1.5px solid rgb(229, 229, 229);
        }
        span.title_content {
            padding-top: 5px;
            display: block;
            text-align: justify;
            padding-bottom: 5px;
            font-size: 17px;
        }
        .performance_container {
            display: flex;
            justify-content: space-around;
        }
        span.dataset {
            display: block;
            font-size: 25px;
            padding-top: 20px;
            padding-bottom: 10px;
            font-weight: 400;
        }
        img.performance_img {
            width: 450px;
        }
        img.group_img {
            width: 1000px;
        }
        .sources_container {
            display: flex;
            justify-content: space-around;
            margin-bottom: 20px;
        }
        img.source_img {
            padding-top: 30px;
            display: block;
            width: 100px;
            transition: 0.5s;
        }
        img.source_img:hover {
            transform: scale(120%, 120%);
            -webkit-transform: scale(120%, 120%);
            -moz-transform: scale(120%, 120%);
        }
        span.source_title {
            display: block;
            margin-top: 15px;
            font-size: 25px;
        }
        .bib {
            text-align: left;
            font-weight: 400;
        }
        pre {
            text-align: left;
            display: block;
            padding: 9.5px;
            margin: 0 0 10px;
            font-size: 13px;
            line-height: 1.428571429;
            color: #333;
            word-break: break-all;
            word-wrap: break-word;
            background-color: #f5f5f5;
            border: 1px solid #ccc;
            border-radius: 4px;
        }
        table.dataset {
            margin: auto;
            border-collapse: collapse;
            border-top: 2px solid black;
            border-bottom: 2px solid black; 
            margin-top: 2px;
        }
        tr.split {
            border-collapse: collapse;
            border-top: 1px solid black;
        }
        tr.model_name {
            font-size: 15px;
        }
        td {
            padding-top: 3px;
            padding-bottom: 3px;
            padding-left: 8px;
            padding-right: 8px;
        }
        th {
            padding-top: 3px;
            padding-bottom: 3px;
            padding-left: 8px;
            padding-right: 8px;
        }
        a {
            text-decoration: none;
        }
        a:visited, a:hover, a:link, a:active {
            color: #333;
        }
        ol.reference_content {
            margin-top: 10px;
            padding-left: 1.2em;
            text-align: justify;
            font-size: 15px;
            /* font-family:  Century Gothic,CenturyGothic,AppleGothic,sans-serif;; */
        }
        li {
            padding-bottom: 15px;
        }
        ul {
            margin-top: 10px;
            padding-left: 1.2em;
            text-align: justify;
            font-size: 17px;
        }
    </style>

</head>
<body>
    <div class="container">
        <div class="header">
            <h3 class="title">Going Deeper into Weak Feature Text Recognition
                in the Wild: A New Benchmark Dataset</h3>
            <p class="authors">Baolong Liu<sup>1</sup> &nbsp;&nbsp;&nbsp;&nbsp;
                Ruiqing Yang<sup>2</sup> &nbsp;&nbsp;&nbsp;&nbsp;
                Wenhao Xu<sup>3</sup>  &nbsp;&nbsp;&nbsp;&nbsp;
                Roukai Huang<sup>4</sup> &nbsp;&nbsp;&nbsp;&nbsp;
                Jiangfeng Dong<sup>5</sup> &nbsp;&nbsp;&nbsp;&nbsp;
                Xun Wang<sup>6</sup>
            </p>
            <p class="affi">¹School of Computer and Information Engineering, Zhejiang Gongshang University</p>
            <!-- <p class="affi">²School of Information Science and Technology, University of Science and Technology of China</p> -->
            <!-- <p class="affi">³Key Lab of Data Engineering and Knowledge Engineering, Renmin University of China</p> -->
        </div>
        <div class="sources">
            <span class="title"></span>
            <div class="sources_container">
                <div class="source">
                    <a href="#" class="source_img" target="_blank">
                        <img src="imgs/paper.svg" alt="" class="source_img">
                    </a>
                    <span class="source_title">Paper</span>
                </div>
                <div class="source">
                    <a href="PRVR_slides.pdf" class="source_img" target="_blank">
                        <img src="imgs/ppt.svg" alt="" class="source_img">
                    </a>
                    <span class="source_title">Slides</span>
                </div>
                <div class="source">
                    <a href="https://github.com/HuiGuanLab/ms-sl/tree/main/dataset" class="source_img" target="_blank">
                        <img src="imgs/data.svg" alt="" class="source_img">
                    </a>
                    <span class="source_title">Data</span>
                </div>
                <div class="source">
                    <a href="https://github.com/HuiGuanLab/ms-sl" class="source_img" target="_blank">
                        <img src="imgs/code.svg" alt="" class="source_img">
                    </a>
                    <span class="source_title">Code</span>
                </div>
            </div>
            <div id="abs"></div>
            <pre ><tt>Todo</tt></pre>
            
        </div>
        <div class="abstract">
            <span class="title">Abstract</span>
            <span class="title_content">
                TODO
            </span>
        </div>
        <div class="formulation">
            <span class="title">Introduction</span>
            <span class="title_content">
                Reading texts from natural images, also referred as
                Scene Text Recognition (STR), is of great importance
                in many application areas. Automatic Ship License Plate
                Recognition (ASLPR) is firstly to localize Ship License Plate
                (SLP) regions in the visible light images, then recognize the
                text in those regions [1]–[4], and is a newly-introduced sub-
                task of STR. As unique identities, SLPs play a significant role
                in ship-to-ship communication, shore-to-ship identification and
                waterway traffic monitoring in occasions such as canlas, ports
                and harbors et al. . A related task is car license plate recog-
                nition [5], which has been studied for years, and is already
                widely used in many real scenes, e.g. autonomous vehicles,
                vehicles tracking and smart city. Just as car license plate
                recognition is to landway transportation, so SLP recognition
                is to waterway transportation. ASLPR is an important issue in
                waterway intelligent transportation systems.
            </span>
            <span class="title_content">
                Recently, a few works concerning the detecting and rec-
                ognizing of SLPs are proposed [1]–[4]. However, the re-
                searches are very preliminary, and no public accessible SLPs
                dataset is released also, hindering the advance of the field.
                Although a number of benchmark datasets regarding STR have
                been proposed [6]–[11] in the past years, it is frequent that,
                when adopted to real applications, the cutting-edge methods
                optimized on the current datasets often show much poorer
                performance or generalization-ability than that reported in the
                literature. For example, the newly presented state-of-the-art
                (SOTA) method in [12] obtains a mean word-level recognition
                accuracy of 96.0% already on six main-stream regular or
                irregular text recognition datasets i.e. IIIT5k [6], CUTE80 [7],
                SVT [8], SVTP [9], ICDAR 2013 [10] and ICDAR 2015 [11],
                but only achieves an accuracy of 35% in our proposed SLPs
                dataset. Obviously, there are huge gaps between research and
                real scenes, trapping those endeavoring in STR field for a long
                time
            </span>
            <!-- <img src="imgs/examples_new.png" alt="" class="group_img">
            <span class="title_content">
                An untrimmed video is considered to be partially relevant w.r.t. a given textual
                query as long as the video contains a (short) moment relevant w.r.t.
                the query, see the figure above. Two textual queries partially relevant to a given video. Only a specific moment in the video is relevant to the
                corresponding query, while the other frames are irrelevant.
            </span> -->
        </div>
        <div class="method">
            <span class="title">Data Visualization</span>
            <!-- <span class="title_content">
                We formulate the PRVR subtask as a MIL problem, simultaneously viewing a video as a bag of video clips and a bag of video
                frames. Clips and frames represent video content at different temporal scales. Based on multi-scale video representation, we propose
                MS-SL to compute the relevance between videos and queries in a
                coarse-to-fine manner. The structure of our proposed model is shown in the figure below.
            </span> -->
            <img src="imgs/data1.svg" alt="" class="">
        </div>
        <div class="performance">
            <span class="title">Performance Comparision</span>
            <span class="title_content">
                <!-- As videos in popular T2VR datasets such as MSR-VTT, MSVD and VATEX are supposed to be fully relevant
                to the queries, they are not suited for our experiments. Here, we
                re-purpose three datasets commonly used for VCMR, <em>i.e.</em>, TVR,
                Activitynet Captions, and Charades-STA, considering their
                natural language queries partially relevant with the corresponding
                videos (a query is typically associated with a specific moment in
                a video). The results on the three datasets are summarized in the tables below. -->
            </span>
            <div class="datasets">
                <span class="title_content">
                    We adopted the state of the art text detection model and text recognition model to carry out relevant experiments on our dataset.
                </span>
                <span class="dataset">Text Detection Algorithm:</span>
                
                <table class="dataset" style="width: 500px;">
                    <tr>
                        <th colspan="5" align="left">Model</th>
                        <th>Year</th>
                        <th>Publisher</th>
                        <th>Precision</th>
                        <th>Recall</th>
                        <th>F-Measure</th>
                    </tr>
            
                    <tr class="model_name split">
                        <td colspan="5" align="left" >DB++ <a href="#w2vv">[1]</a></td>
                        <td>2022</td>
                        <td>TPAMI</td>
                        <td>91.63</td>
                        <td>35.30</td>
                        <td>50.97</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left">I3CL <a href="#hgr">[2]</a></td>
                        <td>2021</td>
                        <td>IJCV</td>
                        <td>49.70</td>
                        <td>60.60</td>
                        <td>54.66</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left">EAST <a href="#htm">[3]</a></td>
                        <td>2017</td>
                        <td>CVPR</td>
                        <td>80.21</td>
                        <td>49.37</td>
                        <td>61.30</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left" >FCENet <a href="#ce">[4]</a></td>
                        <td>2021</td>
                        <td>CVPR</td>
                        <td>80.92</td>
                        <td>50.82</td>
                        <td>62.43</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left">DPText-DETR <a href="#w2vv++">[5]</a></td>
                        <td>2023</td>
                        <td>AAAI</td>
                        <td>69.53</td>
                        <td>72.17</td>
                        <td>70.82</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left" >TESTR <a href="#vse++">[6]</a></td>
                        <td>2022</td>
                        <td>CVPR</td>
                        <td>80.09</td>
                        <td>76.83</td>
                        <td>78.43</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left">TextBPN++ <a href="#de">[7]</a></td>
                        <td>2022</td>
                        <td>TPAMI</td>
                        <td>88.25</td>
                        <td>76.70</td>
                        <td>82.07</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left">PAN <a href="#de++">[8]</a></td>
                        <td>2019</td>
                        <td>ICCV</td>
                        <td>91.76</td>
                        <td>88.20</td>
                        <td>89.94</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="5" align="left">YOLOv5+CSL <a href="#rivrl">[9]</a></td>
                        <td> </td>
                        <td> </td>
                        <td>88.63</td>
                        <td>86.58</td>
                        <td>90.77</td>
                    </tr>
                    <!-- <tr class="split">
                        <td align="left" colspan="8"><em>VCMR models w/o moment localization:</em></td>
                    </tr> -->
                    <tr class="model_name">
                        <td colspan="5" align="left">YOLOv8 <a href="#xml">[10]</a></td>
                        <td> </td>
                        <td> </td>
                        <td>90.93</td>
                        <td>93.83</td>
                        <td>92.14</td>
                    </tr>
                    <!-- <tr class="model_name">
                        <td colspan="5" align="left">ReLoCLNet, SIGIR21 <a href="#relo">[11]</a></td>
                        <td>10.7</td>
                        <td>28.1</td>
                        <td>38.1</td>
                        <td>80.3</td>
                        <td>157.1</td>
                    </tr>
                    <tr class="model_name split">
                        <td colspan="5" align="left">MS-SL(Ours) <a href="#ours">[12]</a></td>
                        <td>13.5</td>
                        <td>32.1</td>
                        <td>43.4</td>
                        <td>83.4</td>
                        <td>172.3</td>
                    </tr> -->
                </table>
            </div>
            <div class="datasets">
                <span class="dataset">Text Recognition Algorithm:</span>
                <table class="dataset" style="width: 550px;">
                    <tr>
                        <th colspan="6" align="left">Method</th>
                        <th>Year</th>
                        <th>Publisher</th>
                        <th>LM</th>
                        <th>Params(M)</th>
                        <th>FPS</th>
                        <th>NED</th>
                        <th>Accuary</th>
                    </tr>
                    <tr class="split">
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left" >CRNN <a href="#w2vv">[1]</a></td>
                        <td>2016</td>
                        <td>TPAMI</td>
                        <td>-</td>
                        <td>8.59</td>
                        <td>144.93</td>
                        <td>43.64</td>
                        <td>5.29</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">TRBA <a href="#htm">[3]</a></td>
                        <td>2019</td>
                        <td>ICCV</td>
                        <td>-</td>
                        <td>33.65</td>
                        <td>44.43</td>
                        <td>64.76</td>
                        <td>30.26</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">ViTSTR <a href="#hgr">[2]</a></td>
                        <td>2021</td>
                        <td>ICDAR</td>
                        <td>-</td>
                        <td>85.83</td>
                        <td>83.33</td>
                        <td>76.67</td>
                        <td>44.60</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left" >SVTR <a href="#rivrl">[9]</a></td>
                        <td>2022</td>
                        <td>IJCAI</td>
                        <td>-</td>
                        <td>22.33</td>
                        <td>69.50</td>
                        <td>80.23</td>
                        <td>45.16</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">SemiMTR <a href="#vse++">[6]</a></td>
                        <td>2022</td>
                        <td>-</td>
                        <td>&#10003;</td>
                        <td>37.81</td>
                        <td>5.92</td>
                        <td>75.13</td>
                        <td>49.76</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left" >PARSeq <a href="#de++">[8]</a></td>
                        <td>2022</td>
                        <td>ECCV</td>
                        <td>-</td>
                        <td>24.20</td>
                        <td>34.13</td>
                        <td>77.86</td>
                        <td>56.52</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">MATRN<a href="#de">[7]</a></td>
                        <td>2022</td>
                        <td>ECCV</td>
                        <td>&#10003;</td>
                        <td>45.77</td>
                        <td>16.93</td>
                        <td>79.74</td>
                        <td>59.39</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">ABINet<a href="#w2vv++">[5]</a></td>
                        <td>2021</td>
                        <td>CVPR</td>
                        <td>&#10003;</td>
                        <td>37.81</td>
                        <td>24.14</td>
                        <td>80.28</td>
                        <td>59.71</td>
                    </tr>
                    <tr class="model_name">
                        <td colspan="6" align="left">MASTER <a href="#ce">[4]</a></td>
                        <td>2021</td>
                        <td>PR</td>
                        <td>-</td>
                        <td>69.86</td>
                        <td>7.86</td>
                        <td>80.53</td>
                        <td>63.17</td>
                    </tr>
                    <tr class="model_name split">
                        <td colspan="6" align="left">Ours<a href="#ours">[12]</a></td>
                        <td>2023</td>
                        <td>-</td>
                        <td>-</td>
                        <td>122</td>
                        <td>34.41</td>
                        <td>85.92</td>
                        <td>66.01</td>
                    </tr>
                </table>
            </div>
        </div>
        <div class="group">
            <span class="title">Our SOTA Algorithm Visualization</span>
            <span class="title_content">
                To gain a further understanding of the individual models, we
                define <em>moment-to-video ratio</em> (M/V) for query, which is measured
                by its corresponding moment's length ratio in the entire video. According
                to M/V, queries can be automatically classified into different groups,
                which enables a fine-grained analysis of how a specific model responds to the different types of queries. 
            </span>
            <img id="mypoly" src="imgs/poly_raw.jpg" alt="" class="group_img">
            <span class="title_content">
                The performance in
                the group with the lowest M/V is the smallest, while the group with
                the highest M/V is the largest. The result allows us to conclude that
                the current video retrieval baseline models better address queries
                of larger relevance to the corresponding video. By contrast, the
                performance we achieved is more balanced in all groups. 
                This result shows that our proposed model is less sensitive to irrelevant
                content in videos.
            </span>
        </div>
        <div class="acknowledgement">
            <span class="title">Acknowledgement</span>
            <span class="title_content" >
                This work was supported by the National
                Key R&D Program of China (2018YFB1404102), NSFC (62172420,
                61902347, 61976188, 62002323), the Public Welfare Technology Research Project of Zhejiang Province (LGF21F020010), the Open
                Projects Program of the National Laboratory of Pattern Recognition,
                the Fundamental Research Funds for the Provincial Universities of
                Zhejiang, and Public Computing Cloud of RUC.
            </span>
        </div>
        <div class="contact">
            <span class="title">Contact</span>
            <ul>
                <li>Baolong Liu: <b><em>liubaolongx@gmail.com</em></li></b> 
                <li>Ruiqing Yang: <b><em>yrq991121@gmail.com</em></li></b> 
            </ul>
        </div>
        <div class="reference">
            <span class="title">Reference</span>
            <ol class="reference_content">
                <li id="w2vv">
                    Jianfeng Dong, Xirong Li, and Cees GM Snoek. 2018. Predicting visual features
                    from text for image and video caption retrieval. <em>IEEE Transactions on Multimedia.</em>
                    20, 12 (2018), 3377–3388.
                </li>
                <li id="hgr">
                    Shizhe Chen, Yida Zhao, Qin Jin, and Qi Wu. 2020. Fine-grained Video-Text
                    Retrieval with Hierarchical Graph Reasoning. <em>In Proceedings of the IEEE/CVF
                    Conference on Computer Vision and Pattern Recognition.</em> 10638–10647.
                </li>
                <li id="htm">
                    Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan
                    Laptev, and Josef Sivic. 2019. HowTo100M: Learning a text-video embedding by
                    watching hundred million narrated video clips. <em>In Proceedings of the IEEE/CVF
                    International Conference on Computer Vision.</em> 2630–2640.
                </li>
                <li id="ce">
                    Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. 2019. Use
                    what you have: Video retrieval using representations from collaborative experts.
                    <em>arXiv preprint arXiv:1907.13487 (2019).</em>
                </li>
                <li id="w2vv++">
                    Xirong Li, Chaoxi Xu, Gang Yang, Zhineng Chen, and Jianfeng Dong. 2019.
                    W2VV++: Fully deep learning for ad-hoc video search. <em>In Proceedings of the 27th
                        ACM International Conference on Multimedia.</em>  1786–1794.
                </li>
                <li id="vse++">
                    Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. 2018. VSE++:
                    Improving visual-semantic embeddings with hard negatives. <em>In Proceedings of
                        the British Machine Vision Conference.</em> 935–943.
                </li>
                <li id="de">
                    Jianfeng Dong, Xirong Li, Chaoxi Xu, Shouling Ji, Yuan He, Gang Yang, and Xun
                    Wang. 2019. Dual encoding for zero-example video retrieval. <em>In Proceedings of
                        the IEEE/CVF Conference on Computer Vision and Pattern Recognition.</em> 9346–9355.
                </li>
                <li id="de++">
                    Jianfeng Dong, Xirong Li, Chaoxi Xu, Xun Yang, Gang Yang, Xun Wang, and
                    Meng Wang. 2021. Dual encoding for video retrieval by text. <em>IEEE Transactions
                        on Pattern Analysis and Machine Intelligence</em>  (2021), 1-1.
                </li>
                <li id="rivrl">
                    Jianfeng Dong, Yabing Wang, Xianke Chen, Xiaoye Qu, Xirong Li, Yuan He, and
                    Xun Wang. 2022. Reading-strategy Inspired Visual Representation Learning
                    for Text-to-Video Retrieval. <em>IEEE Transactions on Circuits and Systems for Video
                        Technology</em> (2022).
                </li>
                <li id="xml">
                    Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. 2020. TVR: A large-scale
                    dataset for video-subtitle moment retrieval. <em>In European Conference on Computer
                        Vision.</em> 447–463.
                </li>
                <li id="relo">
                    Hao Zhang, Aixin Sun, Wei Jing, Guoshun Nan, Liangli Zhen, Joey Tianyi Zhou,
                    and Rick Siow Mong Goh. 2021. Video corpus moment retrieval with contrastive
                    learning. <em>In Proceedings of the 44th International ACM SIGIR Conference on Research
                        and Development in Information Retrieval.</em> 685–695.
                </li>
                <li id="ours">
                    Jianfeng Dong, Xianke Chen, Minsong Zhang, Xun Yang, Shujie Chen, Xirong Li and Xun Wang.
                    2022. Partially Relevant Video Retrieval. <em>In Proceedings of the 30th ACM International Conference on Multimedia.</em>

                </li>
            </ol>
        </div>
      </div> <!-- /container -->
    
</body>

<script>
    var myImage = document.getElementById("mypoly");

    myImage.addEventListener("mouseover", function() {
    this.src = "imgs/poly.jpg";
    });

    myImage.addEventListener("mouseout", function() {
    this.src = "imgs/poly_raw.jpg";
    });
</script>
</html>